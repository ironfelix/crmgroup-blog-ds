"""
SEO Content Gap + Skyscraper Analysis via Keys.so API
======================================================
Ğ”Ğ²Ğ° ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ:
  1. GAP â€” ĞºĞ»ÑÑ‡Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒÑÑ‚ÑÑ, Ğ° Ğ¼Ñ‹ Ğ½ĞµÑ‚ â†’ Ñ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹
  2. SKYSCRAPER â€” Ñ‚Ğ¾Ğ¿-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚Ñ€Ğ°Ñ„Ğ¸ĞºÑƒ â†’ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞµ

Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ API: https://apidoc.keys.so/
Auth: header X-Keyso-TOKEN
Rate limit: 10 req / 10 ÑĞµĞº

Ğ—Ğ°Ğ¿ÑƒÑĞº:
  pip install requests pandas openpyxl
  python seo_gap_analysis.py
"""

import subprocess
import pandas as pd
import time
import json
from pathlib import Path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞĞĞ¡Ğ¢Ğ ĞĞ™ĞšĞ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

API_TOKEN = "699db3c64d1f39.83781556faa89d06f13a4d55eeb785aa412a1007"

# ĞĞ°ÑˆĞ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹
OWN_DOMAINS = [
    "emailsoldiers.ru",
    "crmgroup.ru",
]

# ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹ â€” Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ²Ğ°
AGENCY_COMPETITORS = [
    "emailmatrix.ru",
    "outofcloud.ru",
    "wim.agency",
    "mailfit.ru",
    "inboxmarketing.ru",
    "dirservice.ru",
    "dau.agency",
]

# ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹ â€” Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ (ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ ÑƒĞ³Ğ¾Ğ»)
PLATFORM_COMPETITORS = [
    "mindbox.ru",
    "retailcrm.ru",
    "retailrocket.ru",
]

ALL_COMPETITORS = AGENCY_COMPETITORS + PLATFORM_COMPETITORS

# Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹
MIN_FREQ = 100        # Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ¾Ñ‚ÑĞµĞºĞ°ĞµĞ¼ Ğ¼ÑƒÑĞ¾Ñ€)
MAX_POSITION = 20     # Ğ±ĞµÑ€Ñ‘Ğ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾Ğ¿-20 Ñƒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²
MIN_POSITION_OWN = 21 # Ñƒ Ğ½Ğ°Ñ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ…ÑƒĞ¶Ğµ 20 (Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ)

OUTPUT_DIR = Path("/Users/ivanilin/Documents/ivanilin/01-crmgroup/seo_output")
OUTPUT_DIR.mkdir(exist_ok=True)

BASE_URL = "https://api.keys.so"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API CLIENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

COOKIE_JAR = "/tmp/keyso_cookies_local.txt"

class KeysSoAPI:
    def __init__(self, token: str):
        self.token = token
        self._last_requests = []
        # Warm up: get DDoS-guard cookies
        self._warmup()

    def _warmup(self):
        """ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ cookies Ğ¾Ñ‚ DDoS-guard"""
        subprocess.run([
            "curl", "-s", "-c", COOKIE_JAR, "-b", COOKIE_JAR,
            "-H", f"X-Keyso-TOKEN: {self.token}",
            "-H", "Accept: application/json",
            f"{BASE_URL}/report/simple/domain_dashboard?domain=emailsoldiers.ru"
        ], capture_output=True, text=True, timeout=15)
        time.sleep(1)

    def _rate_limit(self):
        """10 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² 10 ÑĞµĞºÑƒĞ½Ğ´"""
        now = time.time()
        self._last_requests = [t for t in self._last_requests if now - t < 10]
        if len(self._last_requests) >= 10:
            sleep_time = 10 - (now - self._last_requests[0]) + 0.1
            if sleep_time > 0:
                print(f"  â³ rate limit, Ğ¶Ğ´Ñƒ {sleep_time:.1f}Ñ...")
                time.sleep(sleep_time)
        self._last_requests.append(time.time())

    def get(self, endpoint: str, params: dict = None, retries: int = 3) -> dict:
        self._rate_limit()
        qs = "&".join(f"{k}={v}" for k, v in (params or {}).items())
        url = f"{BASE_URL}/{endpoint.lstrip('/')}?{qs}"

        for attempt in range(retries):
            result = subprocess.run([
                "curl", "-s",
                "-c", COOKIE_JAR, "-b", COOKIE_JAR,
                "-H", f"X-Keyso-TOKEN: {self.token}",
                "-H", "Accept: application/json",
                "-H", "Content-Type: application/json",
                "-H", "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
                url
            ], capture_output=True, text=True, timeout=30)

            body = result.stdout.strip()
            if not body:
                wait = (attempt + 1) * 5
                print(f"    âš ï¸ ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ (Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° {attempt+1}/{retries}), Ğ¶Ğ´Ñƒ {wait}Ñ...")
                time.sleep(wait)
                self._warmup()
                continue

            data = json.loads(body)

            # 202 = Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ÑÑ, Ğ¶Ğ´Ñ‘Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµĞ¼
            if isinstance(data, dict) and data.get("code") == 202:
                wait = 30 if attempt == 0 else 60
                print(f"    â³ ĞÑ‚Ñ‡Ñ‘Ñ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ({attempt+1}/{retries}), Ğ¶Ğ´Ñƒ {wait}Ñ...")
                time.sleep(wait)
                continue

            return data

        raise ValueError(f"No data after {retries} retries: {url}")

    def get_context_keywords(self, domain: str, page: int = 1, per_page: int = 100) -> dict:
        """ĞšĞ»ÑÑ‡Ğ¸ Ğ¸Ğ· Ğ¯Ğ½Ğ´ĞµĞºÑ.Ğ”Ğ¸Ñ€ĞµĞºÑ‚ â€” ÑĞ°Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹"""
        return self.get("report/simple/context/keywords", params={
            "domain": domain,
            "base": "ru",
            "current_page": page,
            "per_page": per_page,
        })

    def get_all_context_keywords(self, domain: str, max_pages: int = 5) -> list[dict]:
        """Ğ’ÑĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°"""
        all_kws = []
        for p in range(1, max_pages + 1):
            try:
                data = self.get_context_keywords(domain, page=p)
            except Exception:
                break
            items = data.get("data", [])
            if not items:
                break
            all_kws.extend(items)
            if len(items) < 100:
                break
            time.sleep(0.5)
        return all_kws

    def get_organic_keywords(self, domain: str, page: int = 1, per_page: int = 100) -> dict:
        """ĞÑ€Ğ³Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°"""
        return self.get("report/simple/organic/keywords", params={
            "domain": domain,
            "base": "ru",
            "current_page": page,
            "per_page": per_page,
        })

    def get_top_pages(self, domain: str, page: int = 1, per_page: int = 50) -> dict:
        """Ğ¢Ğ¾Ğ¿ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ ĞºĞ»ÑÑ‡ĞµĞ¹"""
        return self.get("report/simple/organic/pages", params={
            "domain": domain,
            "base": "ru",
            "current_page": page,
            "per_page": per_page,
        })

    def get_all_keywords(self, domain: str, max_pages: int = 5) -> list[dict]:
        """Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ²ÑĞµ ĞºĞ»ÑÑ‡Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° (Ñ Ğ¿Ğ°Ğ³Ğ¸Ğ½Ğ°Ñ†Ğ¸ĞµĞ¹)"""
        all_kws = []
        for p in range(1, max_pages + 1):
            print(f"  {domain}: ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {p}...")
            data = self.get_organic_keywords(domain, page=p)
            items = data.get("data", [])
            if not items:
                break
            all_kws.extend(items)
            if len(items) < 100:
                break
            time.sleep(0.5)
        return all_kws

    def get_all_pages(self, domain: str, max_pages: int = 3) -> list[dict]:
        """Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ¿ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°"""
        all_pages = []
        for p in range(1, max_pages + 1):
            data = self.get_top_pages(domain, page=p)
            items = data.get("data", [])
            if not items:
                break
            all_pages.extend(items)
            if len(items) < 50:
                break
        return all_pages


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 1: GAP-ĞĞĞĞ›Ğ˜Ğ—
# ĞšĞ»ÑÑ‡Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ñƒ Ğ½Ğ°Ñ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_gap_analysis(api: KeysSoAPI):
    print("\nğŸ“Š Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 1: GAP-ĞĞĞĞ›Ğ˜Ğ—")
    print("=" * 50)

    # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ½Ğ°ÑˆĞ¸ ĞºĞ»ÑÑ‡Ğ¸
    print("\nğŸ  Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ½Ğ°ÑˆĞ¸ ĞºĞ»ÑÑ‡Ğ¸...")
    own_keywords = set()
    for domain in OWN_DOMAINS:
        print(f"  â†’ {domain}")
        kws = api.get_all_keywords(domain, max_pages=10)
        for kw in kws:
            own_keywords.add(kw.get("word", "").lower())
    print(f"  Ğ˜Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ²Ğ¾Ğ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ¹: {len(own_keywords)}")

    # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ»ÑÑ‡Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²
    print("\nğŸ¯ Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ»ÑÑ‡Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²...")
    competitor_data = []  # [{key, freq, traff, domain, position, url}, ...]

    for domain in ALL_COMPETITORS:
        print(f"  â†’ {domain}")
        try:
            kws = api.get_all_keywords(domain, max_pages=5)
        except Exception as e:
            print(f"    âš ï¸ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ {domain}: {e}")
            continue
        for kw in kws:
            key = kw.get("word", "").lower()
            pos = kw.get("pos", 99)
            freq = kw.get("ws", 0)  # ws = wordstat frequency

            # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ñƒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ² Ñ‚Ğ¾Ğ¿-20
            if pos <= MAX_POSITION and freq >= MIN_FREQ:
                competitor_data.append({
                    "keyword": key,
                    "frequency": freq,
                    "traffic_competitor": kw.get("wsk", 0),
                    "position_competitor": pos,
                    "competitor_domain": domain,
                    "competitor_url": kw.get("url", ""),
                    "we_have_it": key in own_keywords,
                })

    # GAP = ĞºĞ»ÑÑ‡Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ñƒ Ğ½Ğ°Ñ
    df = pd.DataFrame(competitor_data)
    if df.empty:
        print("  âš ï¸ ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
        return

    gap_df = df[~df["we_have_it"]].copy()

    # ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµĞ¼: Ğ¾Ğ´Ğ¸Ğ½ ĞºĞ»ÑÑ‡ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²
    gap_agg = (
        gap_df.groupby("keyword")
        .agg(
            frequency=("frequency", "max"),
            max_traffic=("traffic_competitor", "max"),
            competitor_count=("competitor_domain", "nunique"),
            competitors=("competitor_domain", lambda x: ", ".join(sorted(set(x)))),
            best_position=("position_competitor", "min"),
            example_url=("competitor_url", "first"),
        )
        .reset_index()
        .sort_values(["competitor_count", "max_traffic"], ascending=[False, False])
    )

    # ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: ĞºĞ»ÑÑ‡ Ñƒ 2+ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² = Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚
    gap_agg["priority"] = gap_agg["competitor_count"].apply(
        lambda x: "ğŸ”´ Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹" if x >= 3 else ("ğŸŸ¡ Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹" if x >= 2 else "ğŸŸ¢ ĞĞ¸Ğ·ĞºĞ¸Ğ¹")
    )

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼
    out_path = OUTPUT_DIR / "gap_analysis.xlsx"
    gap_agg.to_excel(out_path, index=False, sheet_name="GAP")
    print(f"\nâœ… GAP-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {out_path}")
    print(f"   ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ gap-ĞºĞ»ÑÑ‡ĞµĞ¹: {len(gap_agg)}")
    print(f"   Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ (3+ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ°): {len(gap_agg[gap_agg['competitor_count'] >= 3])}")

    # Ğ¢Ğ¾Ğ¿-20 Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ°
    print("\nğŸ” Ğ¢Ğ¾Ğ¿-20 gap-ĞºĞ»ÑÑ‡ĞµĞ¹ (ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°: ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹ + Ñ‚Ñ€Ğ°Ñ„Ğ¸Ğº):")
    print(f"{'ĞšĞ»ÑÑ‡':<50} {'Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°':>8} {'ĞšĞ¾Ğ½ĞºÑƒÑ€.':>7} {'ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚'}")
    print("-" * 80)
    for _, row in gap_agg.head(20).iterrows():
        print(f"{row['keyword']:<50} {row['frequency']:>8,} {row['competitor_count']:>7} {row['priority']}")

    return gap_agg


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 2: SKYSCRAPER
# Ğ¢Ğ¾Ğ¿ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² â†’ Ğ¿Ğ¸ÑˆĞµĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_skyscraper_analysis(api: KeysSoAPI, gap_df_raw: list[dict] = None):
    """Skyscraper Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ keywords Ğ¿Ğ¾ URL â€” Ğ±ĞµĞ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ pages endpoint"""
    print("\n\nğŸ—ï¸  Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 2: SKYSCRAPER (Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ URL)")
    print("=" * 50)

    # ĞĞ°ÑˆĞ¸ URL Ğ¸Ğ· keywords-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    print("\nğŸ  Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ½Ğ°ÑˆĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸Ğ· keywords...")
    own_topics = set()
    for kw in (gap_df_raw or []):
        if kw.get("_domain") in OWN_DOMAINS:
            url = kw.get("url", "")
            slug = url.rstrip("/").split("/")[-1].lower()
            own_topics.add(slug)
    print(f"  ĞĞ°ÑˆĞ¸Ñ… Ñ‚ĞµĞ¼: {len(own_topics)}")

    # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ keywords ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ URL
    print("\nğŸ¯ Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ»ÑÑ‡Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼...")
    pages_data = []

    for domain in ALL_COMPETITORS:
        print(f"  â†’ {domain}")
        try:
            kws = api.get_all_keywords(domain, max_pages=3)
        except Exception as e:
            print(f"    âš ï¸ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ {domain}: {e}")
            continue

        # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ URL
        url_groups: dict = {}
        for kw in kws:
            url = kw.get("url", "/")
            if url not in url_groups:
                url_groups[url] = {"keys": 0, "total_ws": 0, "top10": 0}
            url_groups[url]["keys"] += 1
            url_groups[url]["total_ws"] += kw.get("ws", 0)
            if kw.get("pos", 99) <= 10:
                url_groups[url]["top10"] += 1

        for url, stats in url_groups.items():
            if stats["keys"] < 3:
                continue
            slug = url.rstrip("/").split("/")[-1].lower()
            pages_data.append({
                "competitor_domain": domain,
                "url": domain + url,
                "keywords_count": stats["keys"],
                "total_ws": stats["total_ws"],
                "top10_keys": stats["top10"],
                "slug": slug,
                "we_have_topic": slug in own_topics,
                "is_platform": domain in PLATFORM_COMPETITORS,
            })

    df = pd.DataFrame(pages_data)
    if df.empty:
        print("  âš ï¸ ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
        return

    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼: Ğ½ĞµÑ‚ Ñƒ Ğ½Ğ°Ñ (Ğ¿Ğ¸ÑˆĞ¸ Ğ½Ğ¾Ğ²Ğ¾Ğµ) vs ĞµÑÑ‚ÑŒ Ñƒ Ğ½Ğ°Ñ (ÑƒĞ»ÑƒÑ‡ÑˆĞ¸)
    df["action"] = df["we_have_topic"].map({
        False: "ğŸ“ ĞĞ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğµ",
        True:  "âš¡ ĞĞ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞµ",
    })

    # ĞĞ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ñƒ Ğ½Ğ°Ñ Ğ½ĞµÑ‚ â€” ÑĞ°Ğ¼Ñ‹Ğ¹ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº
    agency_gaps = df[
        (~df["we_have_topic"]) & (~df["is_platform"])
    ].sort_values("traffic", ascending=False)

    # ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ‚Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¼ â€” Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ skyscraper
    platform_skyscraper = df[
        df["is_platform"]
    ].sort_values("traffic", ascending=False).head(30)

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Excel Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ»Ğ¸ÑÑ‚Ğ°Ğ¼Ğ¸
    out_path = OUTPUT_DIR / "skyscraper_analysis.xlsx"
    with pd.ExcelWriter(out_path, engine="openpyxl") as writer:
        df.sort_values("traffic", ascending=False).to_excel(
            writer, index=False, sheet_name="Ğ’ÑĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹"
        )
        agency_gaps.to_excel(
            writer, index=False, sheet_name="GAP Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ²"
        )
        platform_skyscraper.to_excel(
            writer, index=False, sheet_name="Skyscraper vs Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹"
        )

    print(f"\nâœ… Skyscraper-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {out_path}")
    print(f"   Ğ’ÑĞµĞ³Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²: {len(df)}")
    print(f"   ĞĞµÑ‚ Ñƒ Ğ½Ğ°Ñ (Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸): {len(df[~df['we_have_topic']])}")
    print(f"   Ğ•ÑÑ‚ÑŒ Ñƒ Ğ½Ğ°Ñ (ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ): {len(df[df['we_have_topic']])}")

    print("\nğŸ” Ğ¢Ğ¾Ğ¿-15 ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ñƒ Ğ½Ğ°Ñ Ğ½ĞµÑ‚:")
    print(f"{'URL':<65} {'Ğ¢Ñ€Ğ°Ñ„Ğ¸Ğº':>8} {'ĞšĞ»ÑÑ‡ĞµĞ¹':>7}")
    print("-" * 85)
    for _, row in agency_gaps.head(15).iterrows():
        print(f"{row['url']:<65} {row['traffic']:>8,} {row['keywords_count']:>7}")

    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 3: ĞšĞĞĞ¢Ğ•ĞĞ¢-ĞŸĞ›ĞĞ
# ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ gap + skyscraper â†’ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº ÑÑ‚Ğ°Ñ‚ĞµĞ¹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_content_plan(gap_df, skyscraper_df):
    print("\n\nğŸ“‹ Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 3: ĞšĞĞĞ¢Ğ•ĞĞ¢-ĞŸĞ›ĞĞ")
    print("=" * 50)

    rows = []

    # Ğ˜Ğ· GAP â€” Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ñ‹
    if gap_df is not None:
        for _, row in gap_df[gap_df["competitor_count"] >= 2].head(30).iterrows():
            rows.append({
                "Ğ¢Ğ¸Ğ¿": "GAP â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµĞ¼Ğ°",
                "ĞšĞ»ÑÑ‡ / Ğ¢ĞµĞ¼Ğ°": row["keyword"],
                "Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°": row["frequency"],
                "Ğ¢Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»": row["max_traffic"],
                "ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²": row["competitor_count"],
                "ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ URL": row["example_url"],
                "Ğ”Ğ¾Ğ¼ĞµĞ½ Ğ´Ğ»Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸": _suggest_domain(row["keyword"]),
                "ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚": row["priority"],
            })

    # Ğ˜Ğ· Skyscraper â€” Ñ‚Ğ¾Ğ¿ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ²
    if skyscraper_df is not None:
        platform_only = skyscraper_df[
            skyscraper_df["is_platform"] & ~skyscraper_df["we_have_topic"]
        ].head(20)
        for _, row in platform_only.iterrows():
            rows.append({
                "Ğ¢Ğ¸Ğ¿": "SKYSCRAPER â€” Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹",
                "ĞšĞ»ÑÑ‡ / Ğ¢ĞµĞ¼Ğ°": row["url"].rstrip("/").split("/")[-1].replace("-", " "),
                "Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°": "",
                "Ğ¢Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»": row["traffic"],
                "ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²": 1,
                "ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ URL": row["url"],
                "Ğ”Ğ¾Ğ¼ĞµĞ½ Ğ´Ğ»Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸": "emailsoldiers.ru",
                "ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚": "ğŸŸ¡ Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹",
            })

    df = pd.DataFrame(rows)
    if df.empty:
        print("  ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚-Ğ¿Ğ»Ğ°Ğ½Ğ°")
        return

    out_path = OUTPUT_DIR / "content_plan.xlsx"
    df.to_excel(out_path, index=False, sheet_name="ĞšĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚-Ğ¿Ğ»Ğ°Ğ½")
    print(f"âœ… ĞšĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚-Ğ¿Ğ»Ğ°Ğ½ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {out_path}")
    print(f"   Ğ¡Ñ‚Ğ°Ñ‚ĞµĞ¹ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğµ: {len(df)}")


def _suggest_domain(keyword: str) -> str:
    """ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°: ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ»ÑÑ‡Ğ¸ â†’ crmgroup, Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ â†’ emailsoldiers"""
    commercial_signals = ["Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ²Ğ¾", "Ğ·Ğ°ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ", "ÑƒÑĞ»ÑƒĞ³Ğ¸", "ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ", "Ñ†ĞµĞ½Ğ°", "ĞºÑƒĞ¿Ğ¸Ñ‚ÑŒ", "Ğ¿Ğ¾Ğ´ ĞºĞ»ÑÑ‡"]
    kw_lower = keyword.lower()
    if any(s in kw_lower for s in commercial_signals):
        return "crmgroup.ru"
    return "emailsoldiers.ru"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 4: ĞšĞĞœĞœĞ•Ğ Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• Ğ—ĞĞŸĞ ĞĞ¡Ğ«
# Ğ¯Ğ½Ğ´ĞµĞºÑ.Ğ”Ğ¸Ñ€ĞµĞºÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² = Ğ³Ğ¾Ñ€ÑÑ‡Ğ¸Ğµ ĞºĞ»ÑÑ‡Ğ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸ĞºĞ¸
COMMERCIAL_SIGNALS = [
    "Ğ·Ğ°ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ", "Ğ·Ğ°ĞºĞ°Ğ·", "ĞºÑƒĞ¿Ğ¸Ñ‚ÑŒ", "ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ", "Ñ†ĞµĞ½Ğ°", "Ñ†ĞµĞ½Ñ‹", "Ğ¿Ñ€Ğ°Ğ¹Ñ",
    "Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ²Ğ¾", "ÑƒÑĞ»ÑƒĞ³Ğ¸", "ÑƒÑĞ»ÑƒĞ³Ğ°", "Ğ¿Ğ¾Ğ´ ĞºĞ»ÑÑ‡", "Ğ°ÑƒÑ‚ÑĞ¾Ñ€Ñ", "Ğ½Ğ°Ğ½ÑÑ‚ÑŒ",
    "Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°", "Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ", "Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°", "Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ", "Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ",
    "ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ¾Ğ¸Ñ‚", "Ñ‚Ğ°Ñ€Ğ¸Ñ„Ñ‹", "Ñ‚Ğ°Ñ€Ğ¸Ñ„", "ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚", "Ñ„Ñ€Ğ¸Ğ»Ğ°Ğ½Ñ",
]

def run_commercial_analysis(api: KeysSoAPI):
    print("\n\nğŸ’° Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 4: ĞšĞĞœĞœĞ•Ğ Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• Ğ—ĞĞŸĞ ĞĞ¡Ğ« (Ğ”Ğ¸Ñ€ĞµĞºÑ‚)")
    print("=" * 50)

    # ĞĞ°ÑˆĞ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡Ğ¸
    print("\nğŸ  ĞĞ°ÑˆĞ¸ ĞºĞ»ÑÑ‡Ğ¸ Ğ² Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğµ...")
    own_context = set()
    for domain in OWN_DOMAINS:
        kws = api.get_all_context_keywords(domain, max_pages=5)
        for kw in kws:
            own_context.add(kw.get("word", "").lower())
    print(f"  ĞĞ°ÑˆĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… ĞºĞ»ÑÑ‡ĞµĞ¹: {len(own_context)}")

    # ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²
    print("\nğŸ¯ ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² (Ğ”Ğ¸Ñ€ĞµĞºÑ‚)...")
    rows = []
    for domain in ALL_COMPETITORS:
        print(f"  â†’ {domain}")
        try:
            kws = api.get_all_context_keywords(domain, max_pages=5)
        except Exception as e:
            print(f"    âš ï¸ {e}")
            continue
        for kw in kws:
            word = kw.get("word", "").lower()
            rows.append({
                "keyword": word,
                "frequency": kw.get("ws", 0),
                "avg_bid_rub": round(kw.get("avbid", 0) / 100, 0),  # ĞºĞ¾Ğ¿ĞµĞ¹ĞºĞ¸ â†’ Ñ€ÑƒĞ±Ğ»Ğ¸
                "competitor_domain": domain,
                "competitor_url": kw.get("url", ""),
                "we_have_it": word in own_context,
                "is_platform": domain in PLATFORM_COMPETITORS,
            })

    if not rows:
        print("  âš ï¸ ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
        return

    df = pd.DataFrame(rows)

    # Gap: Ñ‡ÑƒĞ¶Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ»ÑÑ‡Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ñƒ Ğ½Ğ°Ñ
    gap = df[~df["we_have_it"]].copy()
    gap_agg = (
        gap.groupby("keyword")
        .agg(
            frequency=("frequency", "max"),
            avg_bid=("avg_bid_rub", "max"),
            competitor_count=("competitor_domain", "nunique"),
            competitors=("competitor_domain", lambda x: ", ".join(sorted(set(x)))),
            example_url=("competitor_url", "first"),
            only_platforms=("is_platform", "all"),
        )
        .reset_index()
        .sort_values(["competitor_count", "avg_bid"], ascending=[False, False])
    )

    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€: Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ»ÑÑ‡Ğ¸
    signals = "|".join(COMMERCIAL_SIGNALS + ["email", "crm", "Ñ€Ğ°ÑÑÑ‹Ğ»Ğº", "Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³"])
    relevant = gap_agg[gap_agg["keyword"].str.contains(signals, na=False)].copy()

    # ĞœĞµÑ‚ĞºĞ°: Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ²Ğ° vs Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹
    relevant["source"] = relevant["only_platforms"].map({
        True: "Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹",
        False: "ĞĞ³ĞµĞ½Ñ‚ÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹"
    })

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼
    out_path = OUTPUT_DIR / "commercial_keywords.xlsx"
    with pd.ExcelWriter(out_path, engine="openpyxl") as writer:
        relevant.sort_values(["competitor_count", "frequency"], ascending=[False, False]).to_excel(
            writer, index=False, sheet_name="ĞšĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ gap"
        )
        gap_agg.to_excel(
            writer, index=False, sheet_name="Ğ’ÑĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ gap"
        )
        df[~df["we_have_it"]].sort_values("frequency", ascending=False).to_excel(
            writer, index=False, sheet_name="Ğ¡Ñ‹Ñ€Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ"
        )

    print(f"\nâœ… ĞšĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ»ÑÑ‡Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {out_path}")
    print(f"   Ğ’ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… gap: {len(gap_agg)}")
    print(f"   Ğ¢ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ…: {len(relevant)}")

    print(f"\nğŸ’° Ğ¢Ğ¾Ğ¿-25 ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ… gap-ĞºĞ»ÑÑ‡ĞµĞ¹:")
    print(f"{'ĞšĞ»ÑÑ‡':<55} {'Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°':>7} {'Ğ¡Ñ‚Ğ°Ğ²ĞºĞ°':>7} {'ĞšĞ¾Ğ½Ğº.':>5} {'Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº'}")
    print("-" * 95)
    for _, r in relevant.sort_values(["competitor_count", "frequency"], ascending=[False, False]).head(25).iterrows():
        print(f"{r['keyword']:<55} {int(r['frequency']):>7,} {int(r['avg_bid']):>6}â‚½ {r['competitor_count']:>5}  {r['source']}")

    return relevant


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    if API_TOKEN == "YOUR_KEYS_SO_TOKEN":
        print("âš ï¸  Ğ£ĞºĞ°Ğ¶Ğ¸ API_TOKEN Ğ¸Ğ· Ğ›Ğš keys.so â†’ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ â†’ API")
        print("   https://www.keys.so/ru/settings/api")
        exit(1)

    api = KeysSoAPI(API_TOKEN)

    print("ğŸš€ SEO Gap + Skyscraper Analysis")
    print(f"   ĞĞ°ÑˆĞ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹: {', '.join(OWN_DOMAINS)}")
    print(f"   ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹-Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ²Ğ°: {len(AGENCY_COMPETITORS)}")
    print(f"   ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹: {len(PLATFORM_COMPETITORS)}")
    print(f"   Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ²: {OUTPUT_DIR}/")

    gap_df = run_gap_analysis(api)
    skyscraper_df = run_skyscraper_analysis(api)
    commercial_df = run_commercial_analysis(api)
    generate_content_plan(gap_df, skyscraper_df)

    print(f"\n\nğŸ‰ Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾!")
    print(f"   ğŸ“ {OUTPUT_DIR}/gap_analysis.xlsx      â€” ĞºĞ»ÑÑ‡Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ñƒ Ğ½Ğ°Ñ")
    print(f"   ğŸ“ {OUTPUT_DIR}/skyscraper_analysis.xlsx â€” Ñ‚Ğ¾Ğ¿-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²")
    print(f"   ğŸ“ {OUTPUT_DIR}/content_plan.xlsx       â€” Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚-Ğ¿Ğ»Ğ°Ğ½")

    print("\n\nğŸ‰ Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾!")
    print(f"   ğŸ“ {OUTPUT_DIR}/gap_analysis.xlsx      â€” ĞºĞ»ÑÑ‡Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ñƒ Ğ½Ğ°Ñ")
    print(f"   ğŸ“ {OUTPUT_DIR}/skyscraper_analysis.xlsx â€” Ñ‚Ğ¾Ğ¿-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²")
    print(f"   ğŸ“ {OUTPUT_DIR}/content_plan.xlsx       â€” Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚-Ğ¿Ğ»Ğ°Ğ½")
